<!DOCTYPE HTML>

<html>
<style>
#p{font-size:6px;


}
</style>
	<head>
		<title>Ali Soltani Nezhad</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						    <h1>
							<div class = "geeks">
							    <img src="Image/oooo.plus_3.png" 
							alt = "Ali Image" />
							</div>
						    </h1>
						<p>Hello everyone!<br />
						</h1>
						<p>Welcome to my personal page. My name is Ali Soltani Nezhad.</p>
						<ul class="actions">

						</ul>
					</div>

				<!-- Header -->

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">About Me</a></li>
<!-- 							<li class="active"><a href="index.html">Kiali Group Inc.</a></li> -->
						</ul>
							
						
						<ul class="icons">
							<li><h3><a href="https://scholar.google.com/citations?user=51VN5s0AAAAJ&hl=en" title="Ali Soltani Nezhad on Google Scholar" target="_blank"><img src="http://yaksoy.github.io/images/logos/scholar.png" alt="Scholar" width="30"></img></a><h3></li>
							<li><h3><a href="https://www.researchgate.net/profile/Ali-Soltani-Nezhad-2" title="Ali Soltani Nezhad on ResearchGate" target="_blank"><img src="http://yaksoy.github.io/images/logos/rgnew.png" alt="RG" width="30"></img></a><h3></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
									<h2><a>Ali Soltani Nezhad</a></h2>
									<p>I am a research assistant at the Computer Vision Center at the Iran University of Science & Technology. 
									I received my master's degree in Electrical Engineering in 2021, supervised by Prof. 
									<a href="https://scholar.google.com/citations?user=sMPEoRcAAAAJ&hl=en">Shahriar Baradaran Shokouhi</a>.</p>
								</header>
							</article>

						<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										<h2><a>Image Action Recognition Using
										Transformer</a></h2>
									</header>
									<a href="" class="image fit"><img src="Image/4th.paper-Page-28.drawio.png" alt="" /></a>
<!-- 									<p>Imbalanced Human Action Recognition Dataset</p> -->
									<ul class="actions special">
<!-- 										<li><a href="https://alisoltani7596.github.io/IHARD46/" class="button">Learn More</a></li> -->
									</ul>
								</article>
								<article>
									<header>
										<h2><a>IHARD46</a></h2>
									</header>
									<a href="https://alisoltani7596.github.io/IHARD46/" class="image fit"><img src="Image/4th.paper-Page-24.drawio.png" alt="" /></a>
									<h3>Imbalanced Human Action Recognition Dataset</h3>
									<ul class="actions special">
										<li><a href="https://alisoltani7596.github.io/IHARD46/" class="button">Learn More</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a>Multi-Expert Human Action Recognition with
										Hierarchical Super-Class Learning</a></h2>
									</header>
									<a href="#" class="image fit"><img src="Image/4th.paper-Page-11.drawio.png" alt="" /></a>
									<p>In human action recognition in still images, many existing studies have leveraged
									extra helpful information along with class labels to mitigate the lack of
									temporal information in still images; However, preparing extra data with manual
									annotation is time-consuming and also prone to human errors. Moreover, the
									existing studies in this domain have not captured semantic information and interrelationship
									among different action classes; that is, they have equally address
									the human activities with different granularities. In human action recognition,
									various human activity classes such as drinking and brushing teeth contain low
									intraclass variance, which could be classified more accurately with a hierarchical
									priors. In this paper, we aim to learn fine-grained structure for human action
									recognition by means of ensemble learning without any extra information. To
									this end, we organize all action activities into a two-phase hierarchy of coarse
									and finer categories. The first coarse phase concentrates on general attributes
									shared by each super-class, whereas the finer phase encodes intricate fine-grained
									details. The training dataset is first categorized into several super-classes and
									a Convolutional Neural Network (CNN) model is trained for each super-class.
									To choose the best configuration of each super-class and characterize intraclass
									and interclass dependency which exists between different action classes inside and outside of each super-class, we propose a novel Graph-based Class Selection
									(GCS) algorithm. Extensive experimental evaluations are conducted on various
									public and private human action recognition datasets, including Stanford40 and
									Pascal VOC2012 and BU101+ datasets. The results demonstrate that the proposed
									method obtains promising improvement in terms of accuracy without any
									auxiliary annotation information while running with fast speed.</p>
									<ul class="actions special">
<!-- 										<li><a href="https://alisoltani7596.github.io/IHARD46/" class="button">Learn More</a></li> -->
									</ul>
								</article>
								<article>
									<header>
										<h2><a>To Transfer or Not To Transfer (TNT):</h2>
										<h3>Action Recognition in Still Image Using Transfer Learning</h3> 
										</a>
									</header>
<!-- 									<a href="#" class="image fit"><img src="Picture2.png" alt="" /></a> -->
									<p>Lack of training data and small datasets is one of the main challenges in human action recognition in still images. For this reason, training a network from scratch does not lead to good results just by using this little training data. Hence, many existing methods use the transfer learning techniques such as fine-tuning a pre-trained network with initial weights from ImageNet. It is important to note that some of the weights obtained from ImageNet are not suitable for human action recognition tasks. On the other hand, in fine-tuning the network, suitable initial weights for human action recognition may be changed. This paper proposes a method called To Transfer or Not To Transfer (TNT) based on knowledge distillation. In this method, a none trainable teacher with ImageNet weights is used to train a light student network for action recognition tasks. In order to transfer relevant knowledge and not to transfer insufficient knowledge from teacher to student, a To Transfer or Not To Transfer Loss (TNTL) function is introduced in this paper. The proposed method is evaluated on Stanford 40 and Pascal VOC datasets, and the results show the superiority of this method over existing methods that use more parameters.</p>
									<ul class="actions special">
<!-- 										<li><a href="https://alisoltani7596.github.io/IHARD46/" class="button">Learn More</a></li> -->
									</ul>
								</article>
								<article>
									<header>
										<h2><a>Still Image Action Recognition Using Ensemble Learning</a></h2>
									</header>
<!-- 									<a href="#" class="image fit"><img src="Saved Pictures.png" alt="" /></a> -->
									<p>In recent years, human action recognition in still images has become a challenge in computer vision. Most methods in this field use annotations such as human and object bounding boxes to determine human-object interaction and pose estimation. Preparing these annotations is time-consuming and costly. In this paper, an ensembling-based method is presented to avoid any additional annotations. According to this fact that a network performance on fewer classes of a dataset is often better than its performance on whole classes; the dataset is first divided into four groups. Then these groups are applied to train four lightweight Convolutional Neural Networks (CNNs). Consequently, each of these CNNs will specialize on a specific subset of the dataset. Then, the final convolutional feature maps of these networks are concatenated together. Moreover, a Feature Attention Module (FAM) is trained to identify the most important features among concatenated features for final prediction. The proposed method on the Stanford40 dataset achieves 86.86% MAP, which indicates this approach can obtain promising performance compared with many existing methods that use annotations.</p>
									<ul class="actions special">
										<li><a href="https://ieeexplore.ieee.org/abstract/document/9443021" class="button">Learn More</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="#">Robust Road Segmentation of Self-Driving Car Using Deep Learning</h2>
									</header>
<!-- 									<a href="#" class="image fit"><img src="Picture1.png" alt="" /></a> -->
									<p>Segmentation of road scenes is a crucial problem in computer vision for autonomous driving. For instance, in order to navigate, an autonomous vehicle needs to determine the drivable area ahead and determine its position on the road with respect to the lane markings. However, the problem is challenging due to the presence of environmental factors like noise, darkness, camera shake, especially in bad weather conditions. In this paper, we present a network with two parts for road segmentation under environmental factors, also in order to simulate environmental factors, we use processed images from KITTI and CamVid datasets. This method is implemented on NVDIA GEFORCE MX150 GPU (4G RAM) and the accuracy arrives at 90.75% under environmental factors.</p>
									<ul class="actions special">
										<li><a href="https://en.civilica.com/doc/1146790/" class="button">Learn More</a></li>
									</ul>
								</article>
							</section>

						<!-- Footer -->

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section class="split contact">
							<section class="alt">
								<h3>Phone</h3>
								<p><a href="#">(915) 901-7154</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="alisoltani7596@gmail.com">alisoltani7596@gmail.com<br /></a>
								     <a href="ali_soltaninezhad@elec.iust.ac.ir">ali_soltaninezhad@elec.iust.ac.ir</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://scholar.google.com/citations?user=51VN5s0AAAAJ&hl=en" class="icon brands fa-skype"><span class="label">Google Scholar</span></a></li>
									<li><a href="https://scholar.google.com/citations?user=51VN5s0AAAAJ&hl=en" class="icon brands fa-telegram"><span class="label">Google Scholar</span></a></li>
									<li><a href="https://scholar.google.com/citations?user=51VN5s0AAAAJ&hl=en" class="icon brands fa-instagram"><span class="label">Google Scholar</span></a></li>
									<li><a href="https://www.researchgate.net/profile/Ali-Soltani-Nezhad-2?ev=hdr_xprf&_sg=YBwR5H3HKVxT72YEQ-v9s7vDdihhUSo4NLBG5Eq1cL8W8aEpYEIR43pQMmM1qF4oAuenghc3hzqSNDM5JIzMLUYc" class="icon brands fa-whatsapp"><span class="label">ResearchGate</span></a></li>
									<li><a href="https://www.linkedin.com/in/ali-soltani-7232b41b3/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
									<li><a href="https://github.com/alisoltani7596" class="icon brands fa-facebook"><span class="label">GitHub</span></a></li>
									<li><a href="https://github.com/alisoltani7596" class="icon brands fa-twitter"><span class="label">GitHub</span></a></li>
									<li><a href="https://github.com/alisoltani7596" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
